
   // for (int s = 0; s < trainingSets; s++) {
    //     printf("Inputs: ");
    //     for (int i = 0; i < nn->input->neuronCount; i++) {
    //         printf("%lf ", trainingInputs[s][i]);
    //     }
    //     printf("\nOutputs: ");
    //     for (int i = 0; i < nn->output->neuronCount; i++) {
    //         printf("%lf ", trainingOutputs[s][i]);
    //     }
    //     printf("\n");
    // }

    /*void OldtrainNN(struct neuralNetwork *nn, const double lr, int epochs, double **trainingInputs, double **trainingOutputs, int trainingSets) {
    assert(nn);
    assert(lr > 0);
    assert(epochs > 0);
 
    // Forward Propgation
    for (int e = 0; e < epochs; e++) {
        printf("EPOCH: %d\n", e);   
        // Shuffle training data
        shuffle(trainingInputs, trainingOutputs, trainingSets);
        // Forward Prop and calculate error of output
        double **deltas = malloc(sizeof(double*) * trainingSets); // error[training sets][output neurons]
        for (int s = 0; s < trainingSets; s++) {
            runNN(nn, trainingInputs[s]);
            deltas[s] = malloc(sizeof(double) * nn->output->neuronCount);
            for (int n = 0; n < nn->output->neuronCount; n++) {
                deltas[s][n] = error(nn->output->neurons[n]->output, trainingOutputs[s][n]);
            } // Currently deltas is set to the error
            // Display progress
            printNN(nn);
            printf("Expected Outputs: ");
            for (int n = 0; n < nn->output->neuronCount; n++) {
                printf("%lf ", trainingOutputs[s][n]);
            }
            printf("\n\n");
        }

        // Do back propogation
        // 3 ways: change bias, change weights, then change last layer activation

    // For each node, it has opinions on what the last node should be
    // If a node is too bright, then it wants all negative in the last to be dimmer
    // And vice versa. So go through each neuron in a layer, sum what it wants the 
    // past layer neurons to be, then change past weights as such
    // This big sum of what each weight should be changed to, for all tests
    // is exactly what the gradient is we are trying to calculate   
    // Stochastic training:
    // Divide up training examples into batches of say, 100
    // compute gradient for each, and do that
    // Drunk man down steps, not a slow man walking 
    // CALCULUS
    // for specific neuron, Cost: C_0 0.5(y - a^(l))^2, y is expected
    // a^(l) = activation {SUM of i [ ( w_i^(l) * a_i^(l-1) ) ] + b^(l)}
    // call this bad boy z^(l) (minus activation)   
    // First let's find derivative of cost in terms of w^(l)
    // this mamma jamma is derivative of z^(l) in terms of w^(l)
    // * derivative of a^(l) in terms of z^(l)
    // * finally derivative of cost in terms of a^(l)   
    // = SUM of i [ (a_i^(l-1) ] 
    // * dActivation(z(l))
    // *  -(y - a^(l)) , dError
    // This was for weight, for bias it is: dActuvation(z(l)) * -(y - a^(l))k   
    // This is for a single example. You then do this for all the training
    // examples and average them. DOUBLE SIGMA  
    // Thus gradient = 1/n SUM of k [ derivative of cost C_k in terms of w^(l) ] to n-1

        // Initialize gradients
        double *outputGradient = malloc(sizeof(double) * nn->output->neuronCount); // dc_da
        double *inputGradient = malloc(sizeof(double) * nn->input->neuronCount);
        if (!outputGradient && !inputGradient) {
            printf("ERROR: Not enough stack memory! output/inputGradient\n");
            assert(0);
        }
        for (int i = 0; i < nn->output->neuronCount; i++) {
            outputGradient[i] = 0;
        }
        for (int i = 0; i < nn->input->neuronCount; i++) {
            inputGradient[i] = 0;
        }

        double **hiddenGradient = malloc(sizeof(double*) * nn->hiddenLayerCount); // hiddenGradient[layer][neuron]
        if (!hiddenGradient) {
            printf("ERROR: Not enough stack memory! hiddenGradient\n");
            assert(0);
        }

        for (int l = 0; l < nn->hiddenLayerCount; l++) {
            hiddenGradient[l] = malloc(sizeof(double) * nn->hidden[l]->neuronCount);
            if (!hiddenGradient) {
                printf("ERROR: Not enough stack memory! hiddenGradient[%d]\n", l);
                assert(0);
            }
            for (int i = 0; i < nn->hidden[l]->neuronCount; i++) {
                hiddenGradient[l][i] = 0;
            }
        }

        // For each training set
        for(int s = 0; s < trainingSets; s++) {
            // WEIGHTS

            // Start with output layer

            // Go through each output neuron and calculate gradient for this training training set
            for (int n = 0; n < nn->output->neuronCount; n++) {
                // Folows chain rule and calculus
                double dz_dw = 0;
                for (int pren = 0; pren < nn->hidden[nn->hiddenLayerCount - 1]->neuronCount; pren++) {
                    dz_dw += nn->hidden[nn->hiddenLayerCount - 1]->neurons[pren]->output;
                }
                double da_dz = dActivation(nn->output->neurons[n]->output);
                double dc_da = dError(nn->output->neurons[n]->output, trainingOutputs[s][n]);
                outputGradient[n] += dz_dw * da_dz * dc_da ;
            }
            
            // Hidden Layers
            for (int l = nn->hiddenLayerCount - 1; l > 0; l--) {
                for (int n = 0; n < nn->hidden[l]->neuronCount; n++) {
                    // Folows chain rule and calculus
                    double dz_dw = 1.0;
                    if (l == 0) {
                        for (int pren = 0; pren < nn->input->neuronCount; pren++) {
                            dz_dw += nn->input->neurons[pren]->output;
                        }
                    } else {
                        for (int pren = 0; pren < nn->hidden[l - 1]->neuronCount; pren++) {
                            dz_dw += nn->hidden[l - 1]->neurons[pren]->output;
                        }
                    }
                    double da_dz = dActivation(nn->hidden[l]->neurons[n]->output);  

                    // This could all be very wrong, idk what I'm doing
                    // Basically this just sums the weights to the next layer and finds their error
                    double dc_da = 0.0;
                    if (l + 1 == nn->hiddenLayerCount) {
                        for (int nextn = 0; nextn < nn->output->neuronCount; nextn++) {
                            dc_da += outputGradient[nextn] * nn->hidden[l]->neurons[n]->weights[nextn];
                        }
                    } else {
                        for (int nextn = 0; nextn < nn->hidden[l + 1]->neuronCount; nextn++) {
                            dc_da += hiddenGradient[l][nextn] * nn->hidden[l]->neurons[n]->weights[nextn];
                        }
                    }

                    // Update gradient with the derivatives by power rule
                    hiddenGradient[l][n] += (dz_dw * da_dz * dc_da);
                }
            }

            // First input layer
            for (int n = 0; n < nn->input->neuronCount; n++) {
                // Folows chain rule and calculus
                double dz_dw = 1.0;
                for (int pren = 0; pren < nn->input->neuronCount; pren++) {
                        dz_dw += trainingInputs[s][pren];
                }
                double da_dz = dActivation(nn->input->neurons[n]->output); 
                double dc_da = 0.0;
                for (int nextn = 0; nextn < nn->hidden[0]->neuronCount; nextn++) {
                    dc_da += hiddenGradient[0][nextn] * nn->hidden[0]->neurons[n]->weights[nextn];
                }
                // Update gradient with the derivatives by power rule
                inputGradient[n] += (dz_dw * da_dz * dc_da);
            }

            // Average and implement weight changes
            for (int n = 0; n < nn->output->neuronCount; n++) {
                outputGradient[n] /= trainingSets;
                nn->output->neurons[n]->bias += outputGradient[n] * lr;
            }
            for (int l = 0; l < nn->hiddenLayerCount; l++) {
                for (int n = 0; n < nn->hidden[l]->neuronCount; n++) {
                    hiddenGradient[l][n] /= trainingSets;
                    nn->hidden[l]->neurons[n]->bias += hiddenGradient[l][n] * lr;
                    int weightCount = 0;
                    if (l + 1 == nn->hiddenLayerCount) {
                        weightCount = nn->output->neuronCount;
                    } else {
                        weightCount = nn->hidden[l + 1]->neuronCount;
                    }
                    for (int w = 0; w < weightCount; w++) {
                        nn->hidden[l]->neurons[n]->weights[w] += hiddenGradient[l][n] * lr;
                    }
                }
            }
            for (int n = 0; n < nn->input->neuronCount; n++) {
                inputGradient[n] /= trainingSets;
                for (int w = 0; w < nn->hidden[0]->neuronCount; w++) {
                    nn->input->neurons[n]->weights[w] += inputGradient[n] * lr;
                }
            }
        }
        // Free allocated deltas and gradients
        for (int s = 0; s < trainingSets; s++) {
            free(deltas[s]);
        }
        free(deltas);
        free(outputGradient);
        free(inputGradient);
        for (int l = 0; l < nn->hiddenLayerCount; l++) {
            free(hiddenGradient[l]);
        }
        free(hiddenGradient);
    }
}*/